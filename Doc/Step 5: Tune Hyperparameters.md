# 第5步：调整超参数
我们必须选择一些超参数来定义和训练模型。 我们依靠直觉，例子和最佳实践建议。 但是，我们首选的超参数值可能无法产生最佳结果。 它只为我们提供了良好的训练起点。 每个问题都是不同的，调整这些超参数将有助于改进我们的模型，以更好地代表手头问题的特殊性。 让我们来看看我们使用的一些超参数以及调整它们的含义：
*  模型中的层数：神经网络中的层数是其复杂性的指标。我们必须谨慎选择这个值。太多的图层将允许模型学习太多关于训练数据的信息，从而导致过度拟合。太少的图层会限制模型的学习能力，导致不合适。对于文本分类数据集，我们使用一层，两层和三层MLP进行了实验。具有两层的模型表现良好，并且在某些情况下优于三层模型。同样，我们尝试了四层和六层的sepCNNs，四层模型表现良好。

*  每层的单位数：图层中的单位必须包含图层执行的变换的信息。对于第一层，这是由功能的数量驱动的。在后续层中，单元数取决于从前一层扩展或收缩表示的选择。尽量减少图层之间的信息丢失。我们尝试了[8,16,32,64]范围内的单位值，并且32/64单位运行良好。

*  dropout率：模型中使用辍学图层进行正则化。它们定义要丢弃的输入分数作为过度拟合的预防措施。推荐范围：0.2-0.5。

*  学习率：这是神经网络权重在迭代之间变化的速率。较大的学习率可能会导致权重大幅波动，我们可能永远找不到最佳值。低学习率是好的，但模型将需要更多迭代才能收敛。从1e-4开始，从低位开始是一个好主意。如果训练非常缓慢，请增加此值。如果您的模型没有学习，请尝试降低学习率。

我们调整了几个特定于我们的sepCNN模型的额外超参数：

*  内核大小：卷积窗口的大小。 推荐值：3或5。

*  嵌入维度：我们想要用来表示字嵌入的维数，即每个单词向量的大小。 推荐值：50-300。 在我们的实验中，我们使用了具有200维度的GloVe嵌入和预先训练的嵌入层。

*  玩这些超参数，看看什么效果最好。 一旦为您的用例选择了性能最佳的超参数，就可以部署您的模型了。
