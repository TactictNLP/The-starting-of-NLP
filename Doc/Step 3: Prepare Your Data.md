# 第3步：准备数据
在将数据提供给模型之前，需要将其转换为模型可以理解的格式。

首先，我们收集的数据样本可能按特定顺序排列。我们不希望任何与样本排序相关的信息影响文本和标签之间的关系。例如，如果数据集按类排序，然后分成训练/验证集，则这些集将不代表数据的整体分布。

确保模型不受数据顺序影响的简单最佳实践是在执行任何其他操作之前始终将数据混洗。如果您的数据已经拆分为培训和验证集，请确保转换验证数据的方式与转换培训数据的方式相同。如果您还没有单独的培训和验证集，您可以在洗牌后拆分样本;通常使用80％的样本进行培训，20％进行验证。

其次，机器学习算法将数字作为输入。这意味着我们需要将文本转换为数字向量。此过程分为两个步骤：

1.```标记化```：将文本分为单词或较小的子文本，这样可以很好地概括文本和标签之间的关系。 这决定了数据集的“词汇表”（数据中存在的唯一token）。

2.```矢量化```：定义一个很好的数值测量来表征这些文本.
让我们看看如何对n-gram向量和序列向量执行这两个步骤，以及如何使用特征选择和规范化技术优化向量表示.

### N-gram载体[选项A]
在随后的段落中，我们将看到如何对n-gram模型进行标记化和矢量化。 我们还将介绍如何使用特征选择和规范化技术优化n-gram表示。

在n-gram向量中，文本被表示为唯一n-gram的集合：n个相邻tokens的组（通常是单词）。 考虑一下文本```The mouse ran up the clock```。 在这里，单词unigrams（n = 1）是 ```['the', 'mouse', 'ran', 'up', 'clock'], the word bigrams (n = 2) are ['the mouse', 'mouse ran', 'ran up', 'up the', 'the clock']```,等等

tokenizer
我们发现，将单词unigrams + bigrams标记为提供良好的准确性，同时减少计算时间。

Vectorization
一旦我们将文本样本分成n-gram，我们需要将这些n-gram转换为我们的机器学习模型可以处理的数值向量。 下面的示例显示了为两个文本生成的unigrams和bigrams分配的索引。

```
Texts: 'The mouse ran up the clock' and 'The mouse ran down'
Index assigned for every token: {'the': 7, 'mouse': 2, 'ran': 4, 'up': 10,
  'clock': 0, 'the mouse': 9, 'mouse ran': 3, 'ran up': 6, 'up the': 11, 'the
clock': 8, 'down': 1, 'ran down': 5}
```
将索引分配给n-gram后，我们通常使用以下选项之一进行矢量化。

One-hot encoding:每个示例文本都表示为一个向量，表示文本中是否存在token中。

```'The mouse ran up the clock' = [1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]```

每个示例文本都表示为一个向量，指示文本中token的计数。 请注意，对应于unigram'the'（下面用粗体）对应的元素现在表示为2，因为单词“the”在文本中出现两次。
```'The mouse ran up the clock' = [1, 0, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1]```

Tf-idf encoding: 上述两种方法的问题在于，在所有文档中以相似频率出现的常用词（即，对数据集中的文本样本不是特别独特的词）不会受到惩罚。 例如，像“a”这样的单词将在所有文本中非常频繁地出现。 因此，对于“the”而言，比其他更有意义的单词更高的token数量并不是非常有用。
```
'The mouse ran up the clock' = [0.33, 0, 0.23, 0.23, 0.23, 0, 0.33, 0.47, 0.33,
0.23, 0.33, 0.33] (See Scikit-learn TdidfTransformer)
```
还有许多其他矢量表示，但以上三种是最常用的。

我们观察到tf-idf编码在准确性方面略优于其他两个（平均：高出0.25-15％），并建议使用此方法对n-gram进行矢量化。 但是，请记住它占用更多内存（因为它使用浮点表示）并且需要更多时间来计算，特别是对于大型数据集（在某些情况下可能需要两倍的时间）。

### 特征选择
当我们将数据集中的所有文本转换为单词uni + bigram标记时，我们最终可能会有数万个标记。 并非所有这些令牌/特征都有助于标签预测。 因此，我们可以删除某些令牌，例如在数据集中极少发生的令牌。 我们还可以测量特征重要性（每个标记对标签预测的贡献程度），并且仅包括信息量最大的标记。

有许多统计函数可以获取特征和相应的标签并输出特征重要性分数。 两个常用的函数是f_classif和chi2。 我们的实验表明，这两个功能同样表现良好。

更重要的是，我们发现许多数据集的精度达到了大约20,000个特征（见图6）。 在此阈值上添加更多功能的贡献非常小，有时甚至会导致过度拟合并降低性能。
```
我们在这里的测试中只使用英文文本。 理想的功能数量可能因语言而异; 这可以在后续分析中探讨。
```
![](https://developers.google.com/machine-learning/guides/text-classification/images/TopKvsAccuracy.svg)
#### 图6：前K特征与精度。 在整个数据集中，精度平稳在20K特征左右。
### 正常化
标准化将所有要素/样本值转换为小值和类似值。 这简化了学习算法中的梯度下降收敛。 从我们所看到的情况来看，数据预处理期间的规范化似乎并没有在文本分类问题中增加太多价值; 我们建议您跳过此步骤。

以下代码汇总了上述所有步骤：

*  将文本样本标记为单词uni + bigrams，
*  使用tf-idf编码进行矢量化，
*  通过丢弃出现少于2次的标记并使用f_classif计算要素重要性，仅从标记向量中选择前20,000个要素。
